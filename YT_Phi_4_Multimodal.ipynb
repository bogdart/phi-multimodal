{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bogdart/phi-multimodal/blob/master/YT_Phi_4_Multimodal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efavTko_IX0q"
      },
      "outputs": [],
      "source": [
        "!pip -q install torch==2.5.1\n",
        "!pip -q install flash_attn==2.7.4.post1\n",
        "!pip -q install transformers==4.48.2\n",
        "!pip -q install accelerate==1.3.0\n",
        "!pip -q install soundfile==0.13.1\n",
        "!pip -q install pillow==11.1.0\n",
        "!pip -q install scipy==1.15.2\n",
        "!pip -q install torchvision==0.21.0\n",
        "!pip -q install backoff==2.2.1\n",
        "!pip -q install peft==0.13.2\n",
        "!pip -q install hf_transfer"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Evd_3CB_K-ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch\n",
        "import os\n",
        "import io\n",
        "from PIL import Image\n",
        "import soundfile as sf\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "sVrXVg_lInNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ ['HF_HUB_ENABLE_HF_TRANSFER'] = '1'"
      ],
      "metadata": {
        "id": "dHzt3D1z8RNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and processor\n",
        "model_path = \"microsoft/Phi-4-multimodal-instruct\"\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    _attn_implementation='eager',\n",
        ").cuda()\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "GdxQx7GRI0Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "1_QYZCitI310"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "dzzrOEQFI4dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "# Upload the image\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file (assuming a single file is uploaded)\n",
        "file_name = list(uploaded.keys())[0]  # Get the first uploaded file's name\n",
        "image_data = uploaded[file_name]      # Get the file's binary content\n",
        "\n",
        "# Open the image from the uploaded bytes\n",
        "image = Image.open(io.BytesIO(image_data))\n",
        "\n",
        "# Optional: Display the image to confirm it worked\n",
        "image.show()  # This opens the image in a viewer (works locally; in Colab, see below)"
      ],
      "metadata": {
        "id": "FX7r6cH8LIyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkZo5mQr8un-"
      },
      "outputs": [],
      "source": [
        "image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"\"\"<|image_1|>\\nYou are a service that converts images of notebooks in mixed English and Russian to Markdown format.\n",
        "Convert line by line.\n",
        "Keep formatting like underscore, bold etc.\n",
        "If there is a picture, describe it.\n",
        "If there is a table, make in Markdown format.\n",
        "If there is a schema, show it in pseudocode.\n",
        "If there is a separator (it is usually ''), replace it with ***.\n",
        "If there is a header as a date, put it as #\n",
        "Don't write any technical text, don't add ```, it will be wrapped in other service.\n",
        "\"\"\"},\n",
        "    # {\"role\": \"assistant\", \"content\": \"The chart displays the percentage of respondents who agree with various statements about their preparedness for meetings. It shows five categories: 'Having clear and pre-defined goals for meetings', 'Knowing where to find the information I need for a meeting', 'Understanding my exact role and responsibilities when I'm invited', 'Having tools to manage admin tasks like note-taking or summarization', and 'Having more focus time to sufficiently prepare for meetings'. Each category has an associated bar indicating the level of agreement, measured on a scale from 0% to 100%.\"},\n",
        "    # {\"role\": \"user\", \"content\": \"Provide insightful questions to spark discussion.\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "6-G02b_FLT8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTSrhj213821"
      },
      "outputs": [],
      "source": [
        "prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "inputs = processor(prompt, [image], return_tensors=\"pt\").to(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5Re8-cJYS5O"
      },
      "outputs": [],
      "source": [
        "generation_args = {\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"do_sample\": False,\n",
        "    # \"temperature\": 0.0,\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMss0xdwYIWH"
      },
      "outputs": [],
      "source": [
        "generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args)\n",
        "\n",
        "# remove input tokens\n",
        "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
        "response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response)"
      ],
      "metadata": {
        "id": "RPbH4WD8MA3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Images"
      ],
      "metadata": {
        "id": "Oq2zpGFSOtKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vqa_phi4(image, prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"<|image_1|>\\n{prompt}\"},\n",
        "    ]\n",
        "    prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = processor(text=prompt, images=[image], return_tensors=\"pt\").to(\"cuda:0\")\n",
        "\n",
        "    generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args)\n",
        "\n",
        "    # remove input tokens\n",
        "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
        "    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "    print(response)\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "g8XKAluNMi2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/bee.JPG?download=true\"\n",
        "\n",
        "response = requests.get(img_url, stream=True)\n",
        "response.raise_for_status()\n",
        "image = Image.open(io.BytesIO(response.content))\n",
        "image"
      ],
      "metadata": {
        "id": "d1Y6Q9zlMzbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is shown in this image?\"\n",
        "\n",
        "vqa_phi4(image, prompt)"
      ],
      "metadata": {
        "id": "8rBb5xyzM-my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What kind of bee is in the image?\"\n",
        "\n",
        "vqa_phi4(image, prompt)"
      ],
      "metadata": {
        "id": "cmLBwNMtNIjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Apart from pink what bright color is another flower?\"\n",
        "\n",
        "vqa_phi4(image, prompt)"
      ],
      "metadata": {
        "id": "6Pm0j2TONsqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = \"/content/planes_phi4.png\"\n",
        "\n",
        "image = Image.open(img_path).convert(\"RGB\")\n",
        "image"
      ],
      "metadata": {
        "id": "mHlm5GWwNznt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is shown in this image?\"\n",
        "\n",
        "vqa_phi4(image, prompt)"
      ],
      "metadata": {
        "id": "rtbF3XVuOJnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"How many planes are there?\"\n",
        "\n",
        "vqa_phi4(image, prompt)"
      ],
      "metadata": {
        "id": "TK3qHC4ufGdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Give me the bounding boxes for the planes\"\n",
        "\n",
        "vqa_phi4(image, prompt)"
      ],
      "metadata": {
        "id": "Tf6T1ov3fLXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"what airport is it?\"\n",
        "\n",
        "vqa_phi4(image, prompt)"
      ],
      "metadata": {
        "id": "9LFHT8cAObjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OCR"
      ],
      "metadata": {
        "id": "rj4qB3wXaL8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = \"/content/blog_text.png\"\n",
        "\n",
        "image = Image.open(img_path).convert(\"RGB\")\n",
        "image"
      ],
      "metadata": {
        "id": "junAXG-Iahjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Please transcribe the text in this\"\n",
        "\n",
        "vqa_phi4(image, prompt)"
      ],
      "metadata": {
        "id": "Cc1ZOyGiamHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"please summarize this text\"\n",
        "\n",
        "vqa_phi4(image, prompt)"
      ],
      "metadata": {
        "id": "OvNbr2F2b2R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"How big is the model?\"\n",
        "\n",
        "vqa_phi4(image, prompt)"
      ],
      "metadata": {
        "id": "YcAyL8bIdwvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio"
      ],
      "metadata": {
        "id": "ZK7akXjpbikf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path = \"/content/mark_zuckerberg_30_01.mp3\"\n",
        "\n",
        "\n",
        "# Read audio file using open()\n",
        "with open(audio_path, \"rb\") as f:\n",
        "    audio, samplerate = sf.read(io.BytesIO(f.read()))\n",
        "\n",
        "from IPython.display import Audio\n",
        "Audio(audio, rate=samplerate)\n"
      ],
      "metadata": {
        "id": "GvilKPVtaB3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio(prompt, audio, samplerate):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"<|audio_1|>\\n{prompt}\"},\n",
        "    ]\n",
        "    prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Process with the model\n",
        "    inputs = processor(text=prompt, audios=[(audio, samplerate)], return_tensors='pt').to('cuda:0')\n",
        "\n",
        "    generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args)\n",
        "\n",
        "    # remove input tokens\n",
        "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
        "    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "    # print(response)\n",
        "    return response"
      ],
      "metadata": {
        "id": "d0UoBUrZeVF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Transcribe the audio\"\n",
        "process_audio(prompt, audio, samplerate)\n"
      ],
      "metadata": {
        "id": "jsX_hsxAgJUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(audio, rate=samplerate)"
      ],
      "metadata": {
        "id": "c948xZ-Hgnqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(audio, rate=samplerate)\n",
        "\n",
        "prompt = \"Transcribe the audio to text, and then translate the audio to French. Use <sep> as a separator between the original transcript and the translation.\"\n",
        "process_audio(prompt, audio, samplerate)"
      ],
      "metadata": {
        "id": "XNYWce94gbSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjGGVLo9brdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Just Text"
      ],
      "metadata": {
        "id": "ZqFbuSmYPHqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def just_text(prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"{prompt}\"},\n",
        "    ]\n",
        "    prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    inputs = processor(text=prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
        "\n",
        "    generate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args)\n",
        "\n",
        "    # remove input tokens\n",
        "    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
        "    response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "\n",
        "    print(response)\n",
        "    return response"
      ],
      "metadata": {
        "id": "5II5h1b7exXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "just_text(\"What is difference between Llama, Vicunas and Alpacas?\")"
      ],
      "metadata": {
        "id": "qsN1zqcdPGTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7BrTJcyZhA3K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}